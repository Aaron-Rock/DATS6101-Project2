---
title: "Project 2"
author: "Jeffrey Hu, Aaron Rock, Sanchit Vijay"
# date: "today"
date: "12/14/2022"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
library(ModelMetrics)
library(pROC)
library(ResourceSelection)
library(pscl)
library(ggplot2)
library(tidyverse)
library(ISLR)
library(dplyr)
# install.packages("matrixStats")
library(matrixStats)
library(psych)
library(hrbrthemes)
library(viridis)
library(ROSE)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# Online Shoppers Dataset  

## Data reading and cleaning  

### Importing the dataset

```{r, results='markup'}
online_shoppers <- data.frame(read.csv("online_shoppers_intention.csv"))
str(online_shoppers)
dim(online_shoppers)
```

### Cleaning up the NAs
There are no NAs in the dataset, and no ommissions in the end.

```{r, results='markup'}
which(is.na(online_shoppers))
online_shoppers <- na.omit(online_shoppers)
str(online_shoppers)
```

### More clean up  
Some of the features are not in their correct data type. We will factorize some of the variables. Some of the factored features have lots of levels, which could be a problem when designing models with dummies and increase computing expense.

```{r, results='markup'}
online_shoppers$OperatingSystems = factor(online_shoppers$OperatingSystems)
online_shoppers$Browser = factor(online_shoppers$Browser)
online_shoppers$Region = factor(online_shoppers$Region)
online_shoppers$TrafficType = factor(online_shoppers$TrafficType)
online_shoppers$VisitorType = factor(online_shoppers$VisitorType)
online_shoppers$Weekend = factor(online_shoppers$Weekend)
online_shoppers$Revenue = factor(online_shoppers$Revenue)
# online_shoppers$Month = factor(online_shoppers$Month)
str(online_shoppers)
```

## EDA Pre-logistic Regression

Let's do an initial mixed corrplot to see the relationship between the features and the possibility of generating revenue. We will only look at the numerical/ordinal factors without the factors.

```{r, results='markup'}
library(corrplot)
online_shoppers_num = subset(online_shoppers, select = -c(Month, OperatingSystems, Browser, Region, TrafficType, VisitorType))
online_shoppers_num$Weekend = as.numeric(online_shoppers_num$Weekend)
online_shoppers_num$Revenue = as.numeric(online_shoppers_num$Revenue)
str(online_shoppers_num)
shop_corr <- cor(online_shoppers_num)
colnames(shop_corr) <- c("Adm", "A_D", "Info", "I_D", "PR", "PR_D", "BR", "ER", "PV", "SDay", "WE", "Rev")
corrplot.mixed(shop_corr)
```

We observed some very obvious correlation between things like the number of administrative pages visited and the amount of time each visitor spent on administrative pages, which is expected just like informational/informational_duration and productedRelated/productRelated_Duration.

The highest correlation with our response is the PageValues, which had a .49 correlation with Revenue. Again, this higher correlation makes sense because the PageValues feature is an average value/metric of the page visited before the user completed a transaction. This is expected, as pagevalues We will do some more EDA on that.

Other than that, the Bounce and Exit Rates both have a negative correlation with the likelihood of generating revenue, while users that visited productRelated pages seems to more likely generate revenue over users that visited admin pages.

From the t-test, we reject the null, and bring strong evidence to support the alternative hypothesis that the PageValue has a strong effect on whether or not the shopper buys.

```{r, results='markup'}
online_shopper_bought <- online_shoppers[which(online_shoppers$Revenue==TRUE),]
online_shopper_not <- online_shoppers[which(online_shoppers$Revenue==FALSE),]
t.test(online_shopper_bought$PageValues, online_shopper_not$PageValues, conf.level = .95)
```


## Logistic Regression

Basic Logistic Regression with PageValues and the three types of pages.

```{r, results='markup'}
unloadPkg("caret")
attach(online_shoppers)
ShopLogit <- glm(Revenue ~ PageValues + ProductRelated + Informational + Administrative + ExitRates, data = online_shoppers, family = "binomial")
summary(ShopLogit)
#kabledply(confusionMatrix(actual=ShopLogit$y,predicted=ShopLogit$fitted.values), title = "Confusion Matrix")
confusionMatrix(actual=ShopLogit$y,predicted=ShopLogit$fitted.values)
prob = predict(ShopLogit, type = "response")
h <- roc(Revenue~prob)
auc(h)
plot(h)
ShopperNullLogit <- glm(Revenue ~ 1, family = "binomial")
mcFadden = 1 - logLik(ShopLogit)/logLik(ShopperNullLogit)
mcFadden
pR2(ShopLogit)
loadPkg("caret")
```

This model looks to be a good model already. The coefficients are all statistically significant aside from the Informational pages, and the AIC is 7675.8. The AUC is .8883, which is higher than 0.8, which is evidence that the predictors are doing well in classifying the dataset.


Let's create testing and training data from Cross Validation to navigate around overfitting and possible variance in creating testing/training data


```{r, results='markup'}
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
ShopModelCV <- train(Revenue ~., data = online_shoppers, method = "glm", family = "binomial", trControl = train_control)
summary(ShopModelCV)
```


```{r, results='markup'}
unloadPkg("caret")
xkabledply(confusionMatrix(actual=ShopModelCV$finalModel$y,predicted=ShopModelCV$finalModel$fitted.values), title = "Confusion Matrix")
prob = predict(ShopModelCV$finalModel, type = "response")
h <- roc(Revenue~prob)
auc(h)
plot(h)
ShopperNullLogit <- glm(Revenue ~ 1, family = "binomial")
mcFadden = 1 - logLik(ShopModelCV$finalModel)/logLik(ShopperNullLogit)
mcFadden
loadPkg("caret")
```

Let's reduce the number of features in this model, particularly the less significant ones. Starting from the null model, let's add the strongest predictors and move in both directions to achieve the best BIC.

```{r, results='markup'}
full = ShopModelCV
null = glm(Revenue ~ 1, family = "binomial")
step(null, scope = list(lower=null, upper=full), direction = "both", criterion = "AIC")
```

Now we have our full logistic model, which is glm(formula = Revenue ~ PageValues + Month + ExitRates + ProductRelated_Duration + TrafficType + VisitorType + ProductRelated, family = "binomial")

Let's run it through the same cross validation training/testing data.

```{r, results='markup'}
ShopModelCV <- train(Revenue ~ PageValues + Month + ExitRates + ProductRelated_Duration + TrafficType + VisitorType + ProductRelated, data = online_shoppers, method = "glm", family = "binomial", trControl = train_control)
summary(ShopModelCV)
```

The new AIC of the model is 7182.9, which is a noticeable improvement on the original full model of 7214.7. Let's apply it to the test data (4:1 on random seed 123) and see how it does.

```{r, results='markup'}
unloadPkg("caret")
xkabledply(confusionMatrix(actual=ShopModelCV$finalModel$y,predicted=ShopModelCV$finalModel$fitted.values), title = "Confusion Matrix")
prob = predict(ShopModelCV$finalModel, type = "response")
h <- roc(Revenue~prob)
auc(h)
plot(h)
ShopperNullLogit <- glm(Revenue ~ 1, family = "binomial")
mcFadden = 1 - logLik(ShopModelCV$finalModel)/logLik(ShopperNullLogit)
mcFadden
```

Finally, let's move the cutoff value to get a high sensitivity without sacrificing too much accuracy. Let's try lowering it to .2.

```{r, results='markup'}
xkabledply(confusionMatrix(actual=ShopModelCV$finalModel$y,predicted=ShopModelCV$finalModel$fitted.values, cutoff=.2), title = "Confusion Matrix")
prob = predict(ShopModelCV$finalModel, type = "response")
h <- roc(Revenue~prob)
auc(h)
plot(h)
ShopperNullLogit <- glm(Revenue ~ 1, family = "binomial")
mcFadden = 1 - logLik(ShopModelCV$finalModel)/logLik(ShopperNullLogit)
mcFadden
```

## Interpretation  

All of the models had generally high accuracies and AUCs, but I would choose the final model as it has the lowest AIC out of the three models. While the McFadden and AUC of the full model was slightly higher than that of the last model, as the complexity is much less with only 7 out of the possible 17 different features.


################################################################################
################################################################################

## Classification Tree and Oversampling

```{r, results='markup'}
#First, load the file
df_orig = data.frame(read.csv("online_shoppers_intention.csv"))
head(df_orig)
```

```{r, results='markup'}
colSums(is.na(df_orig))
```
No null values in the dataset.  

```{r, results='markup'}
table(df_orig$Revenue)
```
We can see there is an imbalance in dataset. True values are less than 20% as compares to False.  

### Factoring categorical variables  
We will factor our categorical variables.  
```{r, results='markup'}
df = data.frame(df_orig)

df$VisitorType = factor(df$VisitorType)
df$Weekend = factor(df$Weekend)
df$Revenue = factor(df$Revenue)
df$Month = factor(df$Month)

df[, c('VisitorType', 'Weekend', 'Revenue', 'Month')] <- sapply(df[, c('VisitorType', 'Weekend', 'Revenue', 'Month')], unclass)

df$VisitorType = factor(df$VisitorType)
df$Weekend = factor(df$Weekend)
df$Revenue = factor(df$Revenue)
df$Month = factor(df$Month)

head(df)
```
### EDA  
We will perform exploratory data analysis for 5 categorical variables: OperatingSystem, Browser, TrafficType, VisitorType, Weekend, and Region and the response variable Revenue.

```{r, results='markup'}
ggplot(df_orig, aes(x=OperatingSystems, color=Revenue, fill=Revenue)) +
  # geom_histogram(bins = 8, fill="pink") + 
  # geom_freqpoly(bins=8, color="red") +
  geom_histogram(binwidth = 0.5, alpha=0.6, position = 'identity') +
  scale_color_viridis(discrete=TRUE) +
  scale_fill_viridis(discrete=TRUE) +
  scale_x_continuous(breaks=1:8) + scale_y_continuous(n.breaks=6) +
  theme_ipsum()  

ggplot(df_orig, aes(x=Browser, color=Revenue, fill=Revenue)) +
  geom_histogram(binwidth = 0.5, alpha=0.6, position = 'identity') +
  scale_x_continuous(breaks=1:13, ) + scale_y_continuous(n.breaks=6) +
  theme_ipsum() 

ggplot(df_orig, aes(x=TrafficType, color=Revenue, fill=Revenue)) +
  geom_histogram(binwidth = 1, alpha=0.6) +
  scale_fill_manual(values=c("orange", "blue")) +
  scale_x_continuous(n.breaks=20) + scale_y_continuous(n.breaks=6) + 
  theme_ipsum()  

ggplot(df_orig, aes(x=VisitorType, color=Revenue, fill=Revenue)) +
  geom_histogram(alpha=0.6, stat="count") +
  scale_fill_manual(values=c("pink", "green")) + scale_color_manual(values=c("pink", "green")) +
  theme_ipsum()  

ggplot(df_orig, aes(x=Weekend, color=Revenue, fill=Revenue)) +
  geom_histogram(alpha=0.6, stat="count") +
  scale_fill_manual(values=c("cyan", "gray")) + scale_color_manual(values=c("cyan", "gray")) +
  theme_ipsum() 

ggplot(df_orig, aes(x=Region, color=Revenue, fill=Revenue)) +
  geom_histogram(binwidth = 1, alpha=0.6, stat="count") + stat_bin(center=1) +
  scale_x_continuous(breaks=1:13, ) + scale_y_continuous(n.breaks=6) + 
  theme_ipsum() 
```

Plotting highlights the imbalance between positive and negative classes in our response variable (Revenue). Also distribution is right skewed.

### Inferential Statistics  
We will perform Chi-Square test on all the last 6 categorical variables.  

```{r, results='markup'}
os_contable = table(df_orig$Revenue, df_orig$OperatingSystems)
os_contable
chisq.test(os_contable, simulate.p.value=T)
```

```{r, results='markup'}
br_contable = table(df_orig$Revenue, df_orig$Browser)
br_contable
chisq.test(br_contable, simulate.p.value=T)
```

```{r, results='markup'}
rg_contable = table(df_orig$Revenue, df_orig$Region)
rg_contable
chisq.test(rg_contable, simulate.p.value=F)
```

```{r, results='markup'}
Tt_contable = table(df_orig$Revenue, df_orig$TrafficType)
Tt_contable
chisq.test(Tt_contable, simulate.p.value=T)
```

```{r, results='markup'}
Vt_contable = table(df_orig$Revenue, df_orig$VisitorType)
Vt_contable
chisq.test(Vt_contable, simulate.p.value=F)
```

```{r, results='markup'}
wk_contable = table(df_orig$Revenue, df_orig$Weekend)
wk_contable
chisq.test(wk_contable, simulate.p.value=F)
```

From above testing, all the variables except Region are statistically significant (p-value < 0.05). Region has p-value of 0.3 (>0.05) making it statistically insignificant. Hence we will omit the Region variable from our modeling.


```{r, results='markup'}
set.seed(1234)
df_sample <- sample(2, nrow(df), replace=TRUE, prob=c(0.80, 0.20))

train <- df[df_sample==1, ]

test <- df[df_sample==2, ]
```

### Classification Tree Modeling and Hyperparameter Tuning  
We will choose "maxdepth" parameter for our decision tree. For this we will be using original unbalanced dataset only.  

```{r, results='markup'}

loadPkg("rpart")
loadPkg("caret")
confusionMatrixResultDf = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )

for (deep in 2:7) {
  tree_fit <- rpart(Revenue ~ . - Region, data=train, method="class", control = list(maxdepth = deep, cp=0.005))
  # 
  cm = confusionMatrix(predict(tree_fit, test, type = "class"), reference = test$Revenue, positive = "2") # from caret library
  # 
  cmaccu = cm$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt = data.frame(Depth=deep, Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf = rbind(confusionMatrixResultDf, cmt)
  # print("Other metrics : ")
}

xkabledply(confusionMatrixResultDf, title="Kyphosis Classification Trees summary with varying MaxDepth")
```

Maxdepth equals 6 is giving highest accuracy, precision, and recall. So we will perform further evaluation using maxdepth as 6.  

```{r, results='markup'}
dtree_model <- rpart(Revenue ~ . - Region, data=train, method="class", control = list(maxdepth = 6, cp=0.005) )
confusionMatrix( predict(dtree_model, test, type = "class"), reference = test[, "Revenue"], positive = "2")
```

We are getting 88.9 as accuracy for above tree model. But we can observe sensitivity or recall is low because it depends on False Negative (FN) that is 182.  

```{r, results='markup'}
plotcp(dtree_model)
```

Above graph of relative error vs. depth/cp shows that after maxdepth equals 6 there is saturation in error.  

### Plotting our tree  
```{r, results='markup'}
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
fancyRpartPlot(dtree_model)
```

```{r, results='markup'}
library(pROC)

prob = predict(dtree_model, test, type = "class")
# df$prob=prob
h <- roc(test$Revenue ~ as.numeric(prob))
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```

Above ROC-AUC analysis shows that imbalance in dataset is not giving good AUC score (<0.8). This is because specificity is much higher than sensitivity.  

### Oversampling dataset
Now we will treat our unbalance dataset. True values are very less as compared to False. So we will oversample True observation of Revenue equal to False value for each corresponding variable in dataset.  

```{r, results='markup'}
train_os = data.frame(train)
train_new <- ovun.sample(Revenue ~ ., data = train_os, method = "over",N = 16710)$data
table(train_new$Revenue)
```

Above table shows that True and False observations are equal.  

### Modeling and Tuning on new balance dataset  

```{r, results='markup'}
confusionMatrixResultDf = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )

for (deep in 2:10) {
  tree_fit <- rpart(Revenue ~ . - Region, data=train_new, method="class", control = list(maxdepth = deep, cp=0.001) )
  # 
  cm = confusionMatrix( predict(tree_fit, test, type = "class"), reference = test[, "Revenue"], positive = "2") # from caret library
  # 
  cmaccu = cm$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt = data.frame(Depth=deep, Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf = rbind(confusionMatrixResultDf, cmt)
  # print("Other metrics : ")
}

xkabledply(confusionMatrixResultDf, title="Kyphosis Classification Trees summary with varying MaxDepth")
```


```{r, results='markup'}
dtree_model <- rpart(Revenue ~ . - Region, data=train_new, method="class", control = list(maxdepth = 2, cp=0.001) )

confusionMatrix(predict(dtree_model, test, type = "class"), reference = test$Revenue, positive = "2")
```

After oversampling our accuracy is 85.7, which is lower than from previous model with unbalanced dataset.  

```{r, results='markup'}
prob = predict(dtree_model, test, type = "class")
# df$prob=prob
h <- roc(test$Revenue ~ as.numeric(prob))
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```

Oversampling causes sensitivity to increase because FN is increased. AUC score is 81.3 now (>0.8).

```{r, results='markup'}
fancyRpartPlot(dtree_model)

unloadPkg("Caret")
```


###########################################################################################################################
###########################################################################################################################

## Aaron' EDA and KNN Model'

```{r, results='hide'}
#First, load the file
online = data.frame(read.csv("online_shoppers_intention.csv"))
# We will use this dataframe later for model manipulation
original_online = data.frame(read.csv("online_shoppers_intention.csv"))
total_observations = nrow(online)
total_observations
head(online)
```
We have read in the data and we have 12,330 observations 18 variables. Just like above we are going to factorize variables that need to be factored fro this part of EDA.
```{r, results='hide'}
# Handling categorical variables prior to plotting
online$Month = as.factor(online$Month)
online$Revenue = as.factor(online$Revenue)
online$Weekend = as.factor(online$Weekend)
online$VisitorType = as.factor(online$VisitorType)
```

# # EDA of Barplots and Density
``` {r}
monthFrequency = ggplot(online, aes(x=Month, color=Month, fill=Month)) + geom_bar() + labs(title="Count of Months",
        x ="Month", y = "Count")
monthFrequency
```
We see a good distribution throughout the months in our  dataset. One thing to notice is that the months April and January are not represented in our dataset. For the barplot above, the months November and May have the highest count. We do not know the product being sold for the site, but there could be a relationship to the product being sold and the month.

``` {r}
userCount = ggplot(online, aes(x=VisitorType, color=VisitorType, fill=VisitorType)) + geom_bar() + labs(title="Visitor Type Count",
        x ="Visitor Types", y = "Count")
userCount
```
Visitor type is important to any successful webpage. For our specific dataset we see from the plot above that at the time when this dataset was taken that their is a considerable amount of returning customers rather seeing a higher bin for the new visitor.

``` {r}
revCount = ggplot(online, aes(x=Revenue, color=Revenue, fill=Revenue)) + geom_bar() + labs(title="Revenue Count",
        x ="Revenue", y = "Count")
revCount
```
The most important variable in our dataset and the variable that will be our target for modeling is revenue. For this variable we can see from the plot above that dataset for this variable is very unbalanced. After viewing the pie chart below the no revenue is at 84.5% and the yes revenue is at 15.5%.

``` {r}
revTrue = subset(online, online$Revenue == TRUE)
revFalse = subset(online, online$Revenue == FALSE)
pieNum <- c(nrow(revTrue), nrow(revFalse))
piepercent<- round(100*pieNum/sum(pieNum), 1)
pie(pieNum, labels = piepercent, main = "Revenue Percentages",col = rainbow(length(pieNum)))
legend("topright", c("True","False"), cex = 0.8, fill = rainbow(length(pieNum)))
```

```{r}
# Special Day data
specialDay = ggplot(online, aes(SpecialDay)) + geom_density(color="darkblue", fill="lightblue") + labs(title="Density Chart of Special Day",x ="Special Day", y = "Density")
specialDay
# scatterSpecialDay = ggplot(online, aes(Revenue, SpecialDay)) + geom_violin(), this plot had no significance
```

The special day variable is a measure of how close to a certified holiday the website was visited on. If the number is 0 this means the visit was not close  and the closer to 1 being the closer to a holiday it is with 1 being on the day of the special day.  When we plot a density chart of this variable we can see that most visits are 0 and the density decreases drastically all the way to 1.

# # Histograms of Individual Pages of the E-commerce Site
```{r}
admin = ggplot(online, aes(Administrative, color=Revenue, fill=Revenue)) + geom_histogram(bins=30) + labs(title="Count of Administrative", x ="Administrative", y = "Count")
admin
info = ggplot(online, aes(Informational, color=Revenue, fill=Revenue)) + geom_histogram(bins=30) + labs(title="Count of Informational", x ="Informational", y = "Count")
info
prod = ggplot(online, aes(ProductRelated, color=Revenue, fill=Revenue)) + geom_histogram(bins=30) + labs(title="Count of Product Related", x ="Product Related", y = "Count")
prod
```
The histograms above are the three pages represent on the e-commerce website. For the x-axis it is representing the number of times the user visited this kind of page. The three different pages for the e-commerce are administrative, informational, and product related. The metric we are trying to see from the plots are if there is a correlation between page and revenue. This is why we plotted the revenue classification by color. Administrative and informational seem to not have as much as an impact as product related pages. When we look at the product related page histogram we see that the revenue limbs up the x-axis more than the other two pages.

```{r}
#
# Duration of the above Pages
#
adminDur = ggplot(online, aes(Administrative_Duration, color=Revenue, fill=Revenue)) + geom_histogram(bins=30) + labs(title="Count of Administrative Duration", x ="Administrative Duration", y = "Count")
adminDur

infoDur = ggplot(online, aes(Informational_Duration, color=Revenue, fill=Revenue)) + geom_histogram(bins=30) + labs(title="Count of Informational Duration", x ="Informational Duration", y = "Count")
infoDur
prodDur = ggplot(online, aes(ProductRelated_Duration, color=Revenue, fill=Revenue)) + geom_histogram(bins=30) + labs(title="Count of Product Related Duration", x ="Product Related Duration", y = "Count")
prodDur
```
Just like the variables above these three variables are still about the three main pages on the e-commerce page but instead of counting how many times a customer visits a certain page the plots above use the duration visited on each page. Similar to the plots we saw with count product related is the only histogram with duration that shows any real relationship between time spent on a page and revenue.

# # Boxplots of User Bounce and Exit Rates, Page Value Plots of Revenue and No Revenue
```{r}
# Bounce Rate is measured by total one page visits divided by total visits 
bounceBox = ggplot(online, aes(BounceRates)) + geom_boxplot(color='blue') + labs(title="Boxplot of Bounce Rate", x ="Bounce Rate")
bounceBox
```
The bounce rate for the entire dataset is extremely low indicating that many users are not only viewing one page when visiting the e-commerce website. The interquartile for this variable is below .025. So, 2.5% of all users are viewing one page only before leaving.The average Bounce Rate for this site is `r mean(online$BounceRates)*100`%.
```{r}
# Exit Rate is measured by total exits from page divided by total visits to page
exitBox = ggplot(online, aes(ExitRates)) + geom_boxplot(color='orange') + labs(title="Boxplot of Exit Rate", x ="Exit Rate")
exitBox
```
For the variable exit rate we are looking to see what is the rate that users are leaving from the site from a specific page. The one down side to our dataset is that the exit rate column is an average of all pages instead of each specific page. With that being said we can see from the boxplot above that the interquartile is right below 0.05. The median looks to be at 0.025. The average Exit Rate for this site is `r mean(online$ExitRates)*100`%. 
```{r}
# Page Value is the average value for a page that a user visited before landing on the goal page or completing an E-commerce transaction
pageBox_rev = ggplot(revTrue, aes(PageValues)) + geom_boxplot(color='green') + labs(title="Boxplot of Page Values with Revenue", x ="Page Values")
pageBox_rev
pageBox_no_rev = ggplot(revFalse, aes(PageValues)) + geom_boxplot(color='red') + labs(title="Boxplot of Page Values of No Revenue Rows", x ="Page Values")
pageBox_no_rev
```
To get a valuable insight for the variable page value we split the dataset by if the observation ended with revenue being produced. When we plotted each new subset we can see a massive difference in the interquartiles being plotted on the boxplot. Page Values that had revenue had an average of `r mean(revTrue$PageValues)`. Page Values that had did not have revenue had an average of `r mean(revFalse$PageValues)`.
# # Corrplot of Variables
```{r}
# Corrplot after changing categorical to numerical
online_new = subset(online, select = -c(VisitorType, Month))
online_new$Weekend = as.numeric(online_new$Weekend)
online_new$Revenue = as.numeric(online_new$Revenue)
corrOnline <- cor(online_new)
corrplot(corrOnline, method="number",number.cex=0.5,tl.cex=.6)
```
After reverting back variables to numeric to plot on the corrplot we are able to see a few variables to target when building our model. Page values has the highest correlation at 0.49.

# # Chi-Squared Test for Months and Special Day against Revenue
```{r}
# Table for Month and Revenue of that Month
monthTable = xtabs( ~ Revenue + Month, data = online)
monthTable
chisqMonth = chisq.test(monthTable)
chisqMonth

# Table for Special Day and Revenue as we get closer to that Day
# This value represents the closeness of the browsing date to special days or holidays
specialDayTable = xtabs( ~ Revenue + SpecialDay, data = online)
specialDayTable
chisqSD = chisq.test(specialDayTable)
chisqSD
```
For my part of EDA I also wanted to test the significance of the variables month and special day in regards to revenue. After building the contingency tables and passing each table to the chi-squared function we can see that these variables are significant to revenue.

```{r, results='hide'}
library(tidyr)
# Rehandle variables for knn model
online$VisitorType = as.factor(online$VisitorType)
online$Weekend = as.integer(online$Weekend)
online$OperatingSystems = as.integer(online$OperatingSystems)
online$Browser = as.integer(online$Browser)
online$TrafficType = as.integer(online$TrafficType)
online$Region = as.integer(online$Region)
online$Month = as.integer(factor(online$Month, levels = month.abb))
online = drop_na(online) # dropping wrong month rows
# Making Visitor Type into numerical variable
library(tidyverse)
online$VisitorType <- str_replace(online$VisitorType,'New_Visitor','1')
online$VisitorType <- str_replace(online$VisitorType,'Other','2')
online$VisitorType <- str_replace(online$VisitorType,'Returning_Visitor','3')
online$VisitorType = as.numeric(online$VisitorType)
```

```{r, results='hide'}
# handling the variables for the specific variables
# PageValues + Month + ExitRates + ProductRelated_Duration + TrafficType + VisitorType + ProductRelated
original_online$VisitorType = as.factor(original_online$VisitorType)
original_online$Weekend = as.integer(original_online$Weekend)
original_online$OperatingSystems = as.integer(original_online$OperatingSystems)
original_online$Browser = as.integer(original_online$Browser)
original_online$TrafficType = as.integer(original_online$TrafficType)
original_online$Region = as.integer(original_online$Region)
original_online$Month = as.integer(factor(original_online$Month, levels = month.abb))
original_online = drop_na(online)
# Making Visitor Type into numerical variable
original_online$VisitorType <- str_replace(original_online$VisitorType,'New_Visitor','1')
original_online$VisitorType <- str_replace(original_online$VisitorType,'Other','2')
original_online$VisitorType <- str_replace(original_online$VisitorType,'Returning_Visitor','3')
original_online$VisitorType = as.numeric(original_online$VisitorType)
original_online = subset(original_online, select = -c(1,2,3,4,7,10,12,13,14,17))
```
# # Splitting Data and Training Data
```{r, results='hide'}
# Scaling
scaledOnline <- as.data.frame(scale(online[1:17], center = TRUE, scale = TRUE))

# Selected Variables based off EDA - Scaling
scaled_Online_select <- as.data.frame(scale(original_online[1:7], center = TRUE, scale = TRUE))

set.seed(123)
online_sample <- sample(2, nrow(scaledOnline), replace=TRUE, prob=c(0.80, 0.20))

# Selected Variables Model based off EDA
set.seed(123)
online_sample_selected <- sample(2, nrow(scaled_Online_select), replace=TRUE, prob=c(0.8, 0.2))

online_training <- scaledOnline[online_sample==1, 1:17]
online_test <- scaledOnline[online_sample==2, 1:17]

# Selected Variables Model based off EDA
online_training_selected <- scaled_Online_select[online_sample_selected==1, 1:7]
online_test_selected <- scaled_Online_select[online_sample_selected==2, 1:7]

online_trainLabels <- online[online_sample==1, 18]
online_testLabels <- online[online_sample==2, 18]

# Selected Variables Model based off EDA
online_trainLabels_selected <- original_online[online_sample_selected==1, 8]
online_testLabels_selected <- original_online[online_sample_selected==2, 8]
```
Before applying our dataset to the model we have split the data into x values and a y target. The y target for this datset is revenue. We did a 4 to 1 split of the dataset and will have two models. One model will utilize every variable on the dataset. The other model is using variables that are significant to the outcome for revenue. After EDA and certain statistical tests the relevant variables are page values, month, exit rates, product related duration, traffic type, visitor type, and product related.
```{r, results='hide'}
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )

ResultDf_selected = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
```
# # KNN Models both Full Model and Specific Variable
```{r}
library(class)
library(gmodels)
for (kval in 5:25) {
  online_pred <- knn(train = online_training, test = online_test, cl=online_trainLabels, k=kval)
  onlinePREDCross <- CrossTable(online_testLabels, online_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  onlinePREDCross
  library(caret)
  cm = confusionMatrix(online_pred, reference = online_testLabels ) 
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
   
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )  
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
``` 

```{r}
# Selected Variable Model
for (kval in 10:30) {
  online_pred_selected <- knn(train = online_training_selected, test = online_test_selected, cl=online_trainLabels_selected, k=kval)
  onlinePREDCross_selected <- CrossTable(online_testLabels_selected, online_pred_selected, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  onlinePREDCross_selected
  
  cm_selected = confusionMatrix(online_pred_selected, reference = online_testLabels_selected ) 
  cmaccu_selected = cm_selected$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu_selected ) )
   
  cmt_selected = data.frame(k=kval, Total.Accuracy = cmaccu_selected, row.names = NULL )  
  ResultDf_selected = rbind(ResultDf_selected, cmt_selected)
  print( xkabledply(   as.matrix(cm_selected), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm_selected$byClass), title=paste("k = ",kval)) )
}
``` 

```{r, results='markup'}
xkabledply(ResultDf, "Total Accuracy Summary")
# Selected Variable Model
xkabledply(ResultDf_selected, "Total Accuracy Summary")
```

# # Accuracy Plot
```{r}
kPlot = ggplot(ResultDf, aes(k, Total.Accuracy)) + geom_point(colour = "blue", size = 3) + geom_line(color='orange')
kPlot

kPlot_selected = ggplot(ResultDf_selected, aes(k, Total.Accuracy)) + geom_point(colour = "blue", size = 3) + geom_line(color='orange')
kPlot_selected
```

# # KNN Model with Optimal K Value
```{r}
# Max Accuracy
online_pred_max <- knn(train = online_training, test = online_test, cl=online_trainLabels, k=15)
onlinePREDCross_max <- CrossTable(online_testLabels, online_pred, prop.chisq = FALSE)

# Selected Variable Model
online_pred_max_selected <- knn(train = online_training_selected, test = online_test_selected, cl=online_trainLabels_selected, k=25)
onlinePREDCross_max_selected <- CrossTable(online_testLabels_selected, online_pred_selected, prop.chisq = FALSE)
```

# # Roc Curve Plots and AUC Scores
```{r}
loadPkg("pROC")
# model with every variable
h <- roc(online_testLabels , as.integer(online_pred_max))
auc(h)
plot(h)
# model with specific variables
h_selected <- roc(online_testLabels_selected , as.integer(online_pred_max_selected))
auc(h_selected)
plot(h_selected)
```

Sadly, both models do not meet the cut off AUC metric of 0.8. We can see from the Confusion Matrix that when the models are at the optimal k value the total accuracy are in the high 80 percent levels. However, the issue we need to address is the specificity that is measured by taking the true negatives divided by the true negatives plus the false positives. For both models we built we can see that there are small specificity rates.
