---
title: "Project 2"
author: "Jeffrey Hu"
# date: "today"
date: "12/4/2022"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
library(ModelMetrics)
library(pROC)
library(ResourceSelection)
library(pscl)
library(ggplot2)
library(tidyverse)
library(ISLR)
library(dplyr)
# install.packages("matrixStats")
library(matrixStats)
library(psych)
library(hrbrthemes)
library(viridis)
library(ROSE)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# Online Shoppers Dataset  

## Data reading and cleaning  

### Importing the dataset

```{r, results='markup'}
online_shoppers <- data.frame(read.csv("online_shoppers_intention.csv"))
str(online_shoppers)
dim(online_shoppers)
```

### Cleaning up the NAs
There are no NAs in the dataset, and no ommissions in the end.

```{r, results='markup'}
which(is.na(online_shoppers))
online_shoppers <- na.omit(online_shoppers)
str(online_shoppers)
```

### More clean up  
Some of the features are not in their correct data type. We will factorize some of the variables. Some of the factored features have lots of levels, which could be a problem when designing models with dummies and increase computing expense.

```{r, results='markup'}
online_shoppers$OperatingSystems = factor(online_shoppers$OperatingSystems)
online_shoppers$Browser = factor(online_shoppers$Browser)
online_shoppers$Region = factor(online_shoppers$Region)
online_shoppers$TrafficType = factor(online_shoppers$TrafficType)
online_shoppers$VisitorType = factor(online_shoppers$VisitorType)
online_shoppers$Weekend = factor(online_shoppers$Weekend)
online_shoppers$Revenue = factor(online_shoppers$Revenue)
# online_shoppers$Month = factor(online_shoppers$Month)
str(online_shoppers)
```

## EDA Pre-logistic Regression

Let's do an initial mixed corrplot to see the relationship between the features and the possibility of generating revenue. We will only look at the numerical/ordinal factors without the factors.

```{r, results='markup'}
library(corrplot)
online_shoppers_num = subset(online_shoppers, select = -c(Month, OperatingSystems, Browser, Region, TrafficType, VisitorType))
online_shoppers_num$Weekend = as.numeric(online_shoppers_num$Weekend)
online_shoppers_num$Revenue = as.numeric(online_shoppers_num$Revenue)
str(online_shoppers_num)
shop_corr <- cor(online_shoppers_num)
corrplot.mixed(shop_corr)
```

We observed some very obvious correlation between things like the number of administrative pages visited and the amount of time each visitor spent on administrative pages, which is expected just like promotional/promotional_duration and productedRelated/productRelated_Duration.

The highest correlation with our response is the PageValues, which had a .49 correlation with Revenue. Again, this higher correlation makes sense because the PageValues feature is an average value/metric of the page visited before the user completed a transaction. This is expected, as pagevalues We will do some more EDA on that.

Other than that, the Bounce and Exit Rates both have a negative correlation with the likelihood of generating revenue, while users that visited productRelated pages seems to more likely generate revenue over users that visited admin pages.

From the t-test, we reject the null, and bring strong evidence to support the alternative hypothesis that the PageValue has a strong effect on whether or not the shopper buys.
```{r, results='markup'}
online_shopper_bought <- online_shoppers[which(online_shoppers$Revenue==TRUE),]
online_shopper_not <- online_shoppers[which(online_shoppers$Revenue==FALSE),]
t.test(online_shopper_bought$PageValues, online_shopper_not$PageValues, conf.level = .95)
```


## Logistic Regression

### Question 7   
Basic Logistic Regression with PageValues and the three types of pages.

```{r, results='markup'}
unloadPkg("caret")
attach(online_shoppers)
ShopLogit <- glm(Revenue ~ PageValues + ProductRelated + Informational + Administrative, data = online_shoppers, family = "binomial")
summary(ShopLogit)
#kabledply(confusionMatrix(actual=ShopLogit$y,predicted=ShopLogit$fitted.values), title = "Confusion Matrix")
confusionMatrix(actual=ShopLogit$y,predicted=ShopLogit$fitted.values)
prob = predict(ShopLogit, type = "response")
h <- roc(Revenue~prob)
auc(h)
plot(h)
ShopperNullLogit <- glm(Revenue ~ 1, family = "binomial")
mcFadden = 1 - logLik(ShopLogit)/logLik(ShopperNullLogit)
mcFadden
pR2(ShopLogit)
loadPkg("caret")
```

This model looks to be a good model already. The coefficients are all statistically significant aside from the Informational pages, and the AIC is 7675.8. The AUC is .8883, which is higher than 0.8, which is evidence that the predictors are doing well in classifying the dataset.


### Question 8  
Let's create testing and training data from Cross Validation to navigate around overfitting and possible variance in creating testing/training data


```{r, results='markup'}
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
ShopModelCV <- train(Revenue ~., data = online_shoppers, method = "glm", family = "binomial", trControl = train_control)
summary(ShopModelCV)
```


```{r, results='markup'}
unloadPkg("caret")
xkabledply(confusionMatrix(actual=ShopModelCV$finalModel$y,predicted=ShopModelCV$finalModel$fitted.values), title = "Confusion Matrix")
prob = predict(ShopModelCV$finalModel, type = "response")
h <- roc(Revenue~prob)
auc(h)
plot(h)
ShopperNullLogit <- glm(Revenue ~ 1, family = "binomial")
mcFadden = 1 - logLik(ShopModelCV$finalModel)/logLik(ShopperNullLogit)
mcFadden
loadPkg("caret")
```

Let's reduce the number of features in this model, particularly the less significant ones. Starting from the null model, let's add the strongest predictors and move in both directions to achieve the best BIC.

```{r, results='markup'}
full = ShopModelCV
null = glm(Revenue ~ 1, family = "binomial")
step(null, scope = list(lower=null, upper=full), direction = "both", criterion = "AIC")
```

Now we have our full logistic model, which is glm(formula = Revenue ~ PageValues + Month + ExitRates + ProductRelated_Duration + TrafficType + VisitorType + ProductRelated, family = "binomial")

Let's run it through the same cross validation training/testing data.

```{r, results='markup'}
ShopModelCV <- train(Revenue ~ PageValues + Month + ExitRates + ProductRelated_Duration + TrafficType + VisitorType + ProductRelated, data = online_shoppers, method = "glm", family = "binomial", trControl = train_control)
summary(ShopModelCV)
```

The new AIC of the model is 7182.9, which is a noticeable improvement on the original full model of 7214.7. Let's apply it to the test data (4:1 on random seed 123) and see how it does.

```{r, results='markup'}
unloadPkg("caret")
xkabledply(confusionMatrix(actual=ShopModelCV$finalModel$y,predicted=ShopModelCV$finalModel$fitted.values), title = "Confusion Matrix")
prob = predict(ShopModelCV$finalModel, type = "response")
h <- roc(Revenue~prob)
auc(h)
plot(h)
ShopperNullLogit <- glm(Revenue ~ 1, family = "binomial")
mcFadden = 1 - logLik(ShopModelCV$finalModel)/logLik(ShopperNullLogit)
mcFadden
```


## Interpretation  

All of the models had generally high accuracies and AUCs, but I would choose the final model as it has the lowest AIC out of the three models. While the McFadden and AUC of the full model was slightly higher than that of the last model, as the complexity is much less with only 7 out of the possible 17 different features.

## Classification Tree and Oversampling

```{r, results='markup'}
#First, load the file
df_orig = data.frame(read.csv("online_shoppers_intention.csv"))
head(df_orig)
```


```{r, results='markup'}
colSums(is.na(df_orig))
```
No null values in the dataset.  

```{r, results='markup'}
table(df_orig$Revenue)
```
We can see there is an imbalance in dataset. True values are less than 20% as compares to False.  

### Factoring categorical variables
```{r, results='markup'}
df = data.frame(df_orig)

df$VisitorType = factor(df$VisitorType)
df$Weekend = factor(df$Weekend)
df$Revenue = factor(df$Revenue)
df$Month = factor(df$Month)

df[, c('VisitorType', 'Weekend', 'Revenue', 'Month')] <- sapply(df[, c('VisitorType', 'Weekend', 'Revenue', 'Month')], unclass)

df$VisitorType = factor(df$VisitorType)
df$Weekend = factor(df$Weekend)
df$Revenue = factor(df$Revenue)
df$Month = factor(df$Month)

head(df)
```
### EDA for last 6 variables  

```{r, results='markup'}
ggplot(df_orig, aes(x=OperatingSystems, color=Revenue, fill=Revenue)) +
  # geom_histogram(bins = 8, fill="pink") + 
  # geom_freqpoly(bins=8, color="red") +
  geom_histogram(binwidth = 0.5, alpha=0.6, position = 'identity') +
  scale_color_viridis(discrete=TRUE) +
  scale_fill_viridis(discrete=TRUE) +
  scale_x_continuous(breaks=1:8) + scale_y_continuous(n.breaks=6) +
  theme_ipsum()  

ggplot(df_orig, aes(x=Browser, color=Revenue, fill=Revenue)) +
  geom_histogram(binwidth = 0.5, alpha=0.6, position = 'identity') +
  scale_x_continuous(breaks=1:13, ) + scale_y_continuous(n.breaks=6) +
  theme_ipsum() 

ggplot(df_orig, aes(x=TrafficType, color=Revenue, fill=Revenue)) +
  geom_histogram(binwidth = 1, alpha=0.6) +
  scale_fill_manual(values=c("orange", "blue")) +
  scale_x_continuous(n.breaks=20) + scale_y_continuous(n.breaks=6) + 
  theme_ipsum()  

ggplot(df_orig, aes(x=VisitorType, color=Revenue, fill=Revenue)) +
  geom_histogram(alpha=0.6, stat="count") +
  scale_fill_manual(values=c("pink", "green")) + scale_color_manual(values=c("pink", "green")) +
  theme_ipsum()  

ggplot(df_orig, aes(x=Weekend, color=Revenue, fill=Revenue)) +
  geom_histogram(alpha=0.6, stat="count") +
  scale_fill_manual(values=c("cyan", "gray")) + scale_color_manual(values=c("cyan", "gray")) +
  theme_ipsum() 

ggplot(df_orig, aes(x=Region, color=Revenue, fill=Revenue)) +
  geom_histogram(binwidth = 1, alpha=0.6, stat="count") + stat_bin(center=1) +
  scale_x_continuous(breaks=1:13, ) + scale_y_continuous(n.breaks=6) + 
  theme_ipsum() 
```
Plotting highlights the imbalance between positive and negative classes in our response variable (Revenue). Also distribution is right skewed.

### Inferential Statistics  
We will perform Chi-Square test on all the last 6 categorical variables.  

```{r, results='markup'}
os_contable = table(df_orig$Revenue, df_orig$OperatingSystems)
os_contable
chisq.test(os_contable, simulate.p.value=T)
```

```{r, results='markup'}
br_contable = table(df_orig$Revenue, df_orig$Browser)
br_contable
chisq.test(br_contable, simulate.p.value=T)
```

```{r, results='markup'}
rg_contable = table(df_orig$Revenue, df_orig$Region)
rg_contable
chisq.test(rg_contable, simulate.p.value=F)
```

```{r, results='markup'}
Tt_contable = table(df_orig$Revenue, df_orig$TrafficType)
Tt_contable
chisq.test(Tt_contable, simulate.p.value=T)
```

```{r, results='markup'}
Vt_contable = table(df_orig$Revenue, df_orig$VisitorType)
Vt_contable
chisq.test(Vt_contable, simulate.p.value=F)
```

```{r, results='markup'}
wk_contable = table(df_orig$Revenue, df_orig$Weekend)
wk_contable
chisq.test(wk_contable, simulate.p.value=F)
```
From above testing, all the variables except Region are statistically significant (p-value < 0.05). Region has p-value of 0.3 (>0.05) making it statistically insignificant. Hence we will omit the Region variable from our modeling.


```{r, results='markup'}
set.seed(1234)
df_sample <- sample(2, nrow(df), replace=TRUE, prob=c(0.80, 0.20))

train <- df[df_sample==1, ]
# test_X <- df[df_sample==2, 1:17]
# 
# train_Y <- df[df_sample==1, 18]
test <- df[df_sample==2, ]
```

### Classification Tree Modeling and Hyperparameter Tuning  
We will choose "maxdepth" parameter for our decision tree. For this we will be using original unbalanced dataset only.  

```{r, results='markup'}
confusionMatrixResultDf = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )

for (deep in 2:7) {
  tree_fit <- rpart(Revenue ~ . - Region, data=train, method="class", control = list(maxdepth = deep, cp=0.005) )
  # 
  cm = confusionMatrix( predict(tree_fit, test, type = "class"), reference = test[, "Revenue"], positive = "2") # from caret library
  # 
  cmaccu = cm$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt = data.frame(Depth=deep, Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf = rbind(confusionMatrixResultDf, cmt)
  # print("Other metrics : ")
}

xkabledply(confusionMatrixResultDf, title="Kyphosis Classification Trees summary with varying MaxDepth")
```

Maxdepth equals 6 is giving highest accuracy, precision, and recall. So we will use to perform further evaluation using maxdepth as 6.  

```{r, results='markup'}
dtree_model <- rpart(Revenue ~ . - Region, data=train, method="class", control = list(maxdepth = 6, cp=0.005) )
confusionMatrix( predict(dtree_model, test, type = "class"), reference = test[, "Revenue"], positive = "2")
```

We are getting 88.9 as accuracy for above tree model. But we can observe sensitivity or recall is low because it depends on False Negative (FN) that is 182.  

```{r, results='markup'}
plotcp(dtree_model)
```
Above graph of relative error vs. depth/cp shows that after maxdepth equals 6 there is saturation in error.  

### Plotting our tree  
```{r, results='markup'}
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
fancyRpartPlot(dtree_model)
```

```{r, results='markup'}
library(pROC)

prob = predict(dtree_model, test, type = "class")
# df$prob=prob
h <- roc(test$Revenue ~ as.numeric(prob))
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```
Above ROC-AUC analysis shows that imbalance in dataset is not giving good AUC score (<0.8). This is because specificity is much higher than sensitivity.  

### Oversampling dataset
Now we will treat our unbalance dataset. True values are very less as compared to False. So we will oversample True observation of Revenue equal to False value for each corresponding variable in dataset.  

```{r, results='markup'}
train_os = data.frame(train)
train_new <- ovun.sample(Revenue ~ ., data = train_os, method = "over",N = 16710)$data
table(train_new$Revenue)
```
Above table shows that True and False observations are equal.  

### Modeling and Tuning on new balance dataset  

```{r, results='markup'}
confusionMatrixResultDf = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )

for (deep in 2:10) {
  tree_fit <- rpart(Revenue ~ . - Region, data=train_new, method="class", control = list(maxdepth = deep, cp=0.001) )
  # 
  cm = confusionMatrix( predict(tree_fit, test, type = "class"), reference = test[, "Revenue"], positive = "2") # from caret library
  # 
  cmaccu = cm$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt = data.frame(Depth=deep, Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf = rbind(confusionMatrixResultDf, cmt)
  # print("Other metrics : ")
}

xkabledply(confusionMatrixResultDf, title="Kyphosis Classification Trees summary with varying MaxDepth")
```


```{r, results='markup'}
dtree_model <- rpart(Revenue ~ . - Region, data=train_new, method="class", control = list(maxdepth = 2, cp=0.001) )

confusionMatrix(predict(dtree_model, test, type = "class"), reference = test$Revenue, positive = "2")
```

After oversampling our accuracy is 85.7, which is lower than from previous model with unbalanced dataset.  

```{r, results='markup'}
prob = predict(dtree_model, test, type = "class")
# df$prob=prob
h <- roc(test$Revenue ~ as.numeric(prob))
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
```
Oversampling causes sensitivity to increase because FN is increased. AUC score is 81.3 now (>0.8).

```{r, results='markup'}
fancyRpartPlot(dtree_model)
```



